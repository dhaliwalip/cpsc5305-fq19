{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression -- Our Goals\n",
    "\n",
    "* Remind ourselves what LR is and what we're trying to do\n",
    "* Define the LR problem for our particular data set\n",
    "* Identify our X and y variables\n",
    "* Use pandas to run a regression\n",
    "* Assess the quality of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be working with a data set from Capital Bikeshare that was used in a [Kaggle competition](https://www.kaggle.com/c/bike-sharing-demand/data).\n",
    "\n",
    "The objective of the competition is to predict total ridership of Capital Bikeshare in any given hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Dictionary (from Kaggle)\n",
    "\n",
    "| Variable| Description |\n",
    "|---------|----------------|\n",
    "|datetime| hourly date + timestamp  |\n",
    "|season|  1=winter, 2=spring, 3=summer, 4=fall |\n",
    "|holiday| whether the day is considered a holiday|\n",
    "|workingday| whether the day is neither a weekend nor holiday|\n",
    "|weather| 1 -> Clear or partly cloudy;  2 -> Clouds and mist; 3 -> Light rain or snow;  4 -> Heavy rain or snow|\n",
    "|temp| temperature in Celsius|\n",
    "|atemp| \"feels like\" temperature in Celsius|\n",
    "|humidity| relative humidity|\n",
    "|windspeed| wind speed|\n",
    "|casual| number of non-registered user rentals initiated|\n",
    "|registered| number of registered user rentals initiated|\n",
    "|count| number of total rentals|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"read-in-the--capital-bikeshare-data\"></a>\n",
    "### Read In the Capital Bikeshare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and set the datetime as the index.\n",
    "#  -- We sometimes want to do this if (a) that column is unique\n",
    "#       and acts as an ID, and (b) we don't want to use it as a\n",
    "#       predictor\n",
    "#  -- Notice also forcing it to be a real date/time instead of string\n",
    "bikes = pd.read_csv('bikeshare.csv', index_col='datetime', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bikes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first five rows of the DataFrame.\n",
    "bikes.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:120%\">What does each observation represent?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:120%\">What is the response variable (as defined by Kaggle)?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size:120%\">How many features are there that could be predictors in a regression?  What are other potential features?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename some variables\n",
    "\n",
    "\"count\" is a method in Pandas (and a very non-specific name), so it's best to name that column something else\n",
    "\n",
    "In general, you may want to rename columns if it is not obvious what might be stored in them, and you might also adopt the convention of including a column's units or type in its name. Although we will only rename the target column here, a few examples might be to rename:\n",
    "\n",
    "| old name | new name |\n",
    "| ---    | --- |\n",
    "| temp | temp_celsius\n",
    "| windspeed | windspeed_knots\n",
    "| casual | num_casual_users\n",
    "| registered | num_registered_users\n",
    "| season | season_num\n",
    "| holiday | is_holiday\n",
    "| workingday | is_workingday\n",
    "| humidity | humidity_percent\n",
    "\n",
    "This names should make it obvious what is stored in each column. The downside is slightly longer column names, which could affect table readability in Jupyter. It would be ideal to use very specific names in CSV files to assist others reading them. In your own code, use whatever makes sense for your work -- if you are viewing lots of Pandas tables, you may want to use shorter names. However, readable specific names are preferred in Python code since it prevents mistakes and makes your work more clear to readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<span style=\"color:blue;font-size:120%\">Use the .rename() method to rename count to total_rentals</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-the-data\"></a>\n",
    "### Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have a general feeling for what the data looks like before building a model. Ideally, before creating the model you would have some sense of which variables might matter most to predict the response. This dataset is fairly intuitive (and the purpose of this lesson is not visualization), so we will keep the visualization short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(bikes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to focus on the single variate ```temp``` first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.atemp.plot(kind=\"box\")\n",
    "bikes.atemp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes.atemp.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas scatterplot\n",
    "bikes.plot(kind='scatter', x='temp', y='total_rentals', alpha=0.2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn scatterplot with regression line\n",
    "#    aspect => plot is twice as long as it is high\n",
    "#    alpha => transparancy on scale of 0 (transparent) to 1 (opaque)\n",
    "\n",
    "sns.lmplot(x='temp', y='total_rentals', data=bikes, aspect = 2.0, scatter_kws={'alpha':0.2});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Basics\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"form-of-linear-regression\"></a>\n",
    "### Form of Linear Regression\n",
    "\n",
    "Recall that each model always contains some amount of random irreducible error $\\epsilon$. So, given a prediction $\\hat{y}$, the actual $y = \\hat{y} + \\epsilon$. Below, we will assume $y$ is exactly linear.\n",
    "\n",
    "- We are often taught the formula for a line is: $y = mx + b$.\n",
    "- But it's more common in the literature to use Greek letters for coefficients: $y = \\alpha + \\beta X$.\n",
    "\n",
    "---\n",
    "\n",
    "Here, we will generalize this to $n$ independent variables as follows:\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon\\quad\\quad =\\quad\\quad (\\sum_{i=0}^{n} \\beta_ix_i) + \\epsilon$$\n",
    "\n",
    "- $y$ is the response.\n",
    "- $\\beta_0$ is the intercept.\n",
    "- $\\beta_1$ is the coefficient for $x_1$ (the first feature).\n",
    "- $\\beta_n$ is the coefficient for $x_n$ (the nth feature).\n",
    "- $\\epsilon$ is the _error_ term which is independent of x, y, $i$, and $n$\n",
    "\n",
    "A practical example of this applied to our data might be:\n",
    "\n",
    "$total\\_rides = 20 + -2 \\cdot temp + -3 \\cdot windspeed\\ +\\ ...\\ +\\ 0.1 \\cdot registered$\n",
    "\n",
    "This equation is still called **linear** because the highest degree of the independent variables (e.g. $x_i$) is 1. Note that because the $\\beta$ values are constants, they will not be independent variables in the final model, as seen above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:120%\"> What are the limits of this linearity assumption?  What kind of relationships can't we capture?  Can you think of real examples where linearity is violated?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  Non-linear such as squared or exponential\n",
    "* Interrelationship terms -- X1 * X2\n",
    "* Definitional relationships   X1 AND X2\n",
    "* Contribution of X to Y changes with the value of Y -- for example humidity affects ridership more on hot days than on cold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In the regression equation\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n + \\epsilon$\n",
    "\n",
    "the $\\beta$ values are called the **model coefficients**:\n",
    "\n",
    "- These values are estimated (or \"learned\") during the model fitting process using the **least squares criterion**.\n",
    "- Specifically, we are trying to find the line (mathematically) that minimizes the **sum of squared residuals** (or \"sum of squared errors\").\n",
    "- Once we've learned these coefficients, we can use the model to predict the response.\n",
    "\n",
    "![Estimating coefficients](estimating_coefficients.png)\n",
    "\n",
    "Remember the book defines\n",
    "$$MSE = \\frac{1} {n} \\| \\hat{y}(\\mathbf{X}) - \\vec{y} \\|$$\n",
    "\n",
    "and said the algorithm chooses coefficients that minimize this quantity. Same concept with different notation.  (Also if you are minimizing a quantity $E$ the same value will minimize $E/2$.)\n",
    "\n",
    "In the diagram above:\n",
    "\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the vertical distances between the observed values and the least squares line.\n",
    "\n",
    "If there are two predictors, the model fits a plane, and residuals are the distance between the observed value and the least squares plane.\n",
    "\n",
    "![Regression with Two Variates](multiple_regression_plane.png)\n",
    "\n",
    "If there are more than two predictors, it's hard to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Linear regression is an example of \"supervised learning\" -- this is learning some model on the basis of *training examples* that are *labeled* with the correct outcome value. \n",
    "In our example, our $i^{th}$ training example looks like this mathematically\n",
    "\n",
    "$$(x_{i,1} x_{i,2}, ..., x_{i,n}, y)$$\n",
    "\n",
    "but sometimes the $x$ and $y$ are represented separately and in matrix notation look like this for a situation where we have $m$ independent variables and $n$ observations\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix}\n",
    "x_{1,1} & x_{1,2}, \\ldots, & x_{1, m}\\\\\n",
    "x_{2,1} & x_{2,2}, \\ldots, & x_{2, m}\\\\\n",
    "\\ldots\\\\\n",
    "x_{n,1} & x_{n,2}, \\ldots, & x_{n, m}\n",
    "\\end{bmatrix}\n",
    "\\quad\\quad\\quad\n",
    "\\mathbf{Y} = \n",
    "\\begin{bmatrix}\n",
    "y_1\\\\\n",
    "y_2\\\\\n",
    "\\ldots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"requirements-for-working-with-data-in-scikit-learn\"></a>\n",
    "### Requirements for Working With Data in scikit-learn\n",
    "\n",
    "1. Features and response should be separate objects.\n",
    "2. Features and response should be entirely numeric.\n",
    "3. Features and response should be NumPy arrays (or easily converted to NumPy arrays).\n",
    "4. Features and response should have specific shapes (outlined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"building-a-linear-regression-model-in-sklearn\"></a>\n",
    "### Building a (Single) Linear Regression Model in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a feature matrix called X that holds a `DataFrame` with only the temp variable and a `Series` called y that has the \"total_rentals\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "feature_cols = ['temp']\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total_rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:120%\">Why do you think X is capitalized but y is not?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "print(type(y))\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"scikit-learns--step-modeling-pattern\"></a>\n",
    "### scikit-learn's Four-Step Modeling Pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import the class you plan to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** *Instantiate* the *estimator.*\n",
    "\n",
    "- Estimator is scikit-learn's term for model.\n",
    "- Instantiate is an object-oriented programming term meaning \"make an instance of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an instance of a LinearRegression object.\n",
    "lr = LinearRegression()\n",
    "type(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Created an object that \"knows\" how to do linear regression, and is just waiting for data.\n",
    "- Name of the object does not matter.\n",
    "- All parameters not specified are set to their defaults.\n",
    "- Can specify tuning parameters (aka \"hyperparameters\") during this step. \n",
    "\n",
    "To view the possible parameters, either use the `help` built-in function or evaluate the newly instantiated model, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** Fit the model with data (aka \"model training\").\n",
    "\n",
    "- Model is \"learning\" the relationship between X and y in our \"training data.\"\n",
    "- Process through which learning occurs varies by model.\n",
    "- Occurs in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Once a model has been fit with data, it's called a \"fitted model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Predict the response for a new observation.\n",
    "\n",
    "- New observations are called \"out-of-sample\" data.\n",
    "- Uses the information it learned during the model training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we predict at x=0 we will get the intercept\n",
    "lr.predict(np.array([0]).reshape(-1,1))\n",
    "lr.predict([[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(np.array([0]).reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Just for convenience -- take a number (x) gets its predicted y\n",
    "def lrpred(xval):\n",
    "    return lr.predict(np.array([xval]).reshape(1,-1))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lrpred(4) - lrpred(3))\n",
    "print(lrpred(21) - lrpred(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict multiple x values at once\n",
    "\n",
    "X_new = [[4], [3],[21],[20]]\n",
    "lr.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ask the model to make three predictions, at temp values 0, 10, 100. To do this, our feature matrix is always a 2-D array where each row is a list of features. Since we only have a single feature, the temperature, each row will contain only a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict([[0], [10], [100]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we just predicted using our model is, \"If the temperature is 0 degrees, the total number of bike rentals will be ~6.046, and if the temperature is 10 degrees the total number of bike rentals will ~97.751.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size=120%\">How would you informally confirm that these are reasonable predictions?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make sure they obey the regression equation\n",
    "* Look for observations \"close\" to them in the data set and see if the rentals value is about the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the intercept ($\\beta_0$):\n",
    "\n",
    "- It is the value of $y$ when all independent variables are 0.\n",
    "- Here, it is the estimated number of rentals when the temperature is 0 degrees Celsius.\n",
    "- <span style=\"color:blue\">**Note:** It does not always make sense to interpret the intercept. (Why?)</span>\n",
    "\n",
    "Interpreting the \"temp\" coefficient ($\\beta_1$):\n",
    "\n",
    "- **Interpretation:** An increase of 1 degree Celcius is _associated with_ increasing the number of total rentals by $\\beta_1$.\n",
    "- Here, a temperature increase of 1 degree Celsius is _associated with_ a rental increase of 9.17 bikes.\n",
    "- This is not a statement of causation.\n",
    "- $\\beta_1$ would be **negative** if an increase in temperature was associated with a **decrease** in total rentals.\n",
    "- $\\beta_1$ would be **zero** if temperature is not associated with total rentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:120%\">How many bike rentals would we predict if the temperature was 25 degrees Celsius?  Do the calculation both manually, and using the model.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually calculate the prediction.\n",
    "linreg.intercept_  +   linreg.coef_ * [25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict([[25]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"does-the-scale-of-the-features-matter\"></a>\n",
    "### Does the Scale of the Features Matter?\n",
    "\n",
    "Let's say that temperature was measured in Fahrenheit, rather than Celsius. How would that affect the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for Fahrenheit temperature.\n",
    "bikes['temp_F'] = bikes.temp * 1.8 + 32\n",
    "bikes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn scatterplot with regression line\n",
    "sns.lmplot(x='temp_F', y='total_rentals', data=bikes, aspect=1.5, scatter_kws={'alpha':0.2});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='temp', y='total_rentals', data=bikes, aspect=1.5, scatter_kws={'alpha':0.2});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuild the `LinearRegression` from above using the `temp_F` features instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "feature_cols = ['temp_F']\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total_rentals\n",
    "\n",
    "# Instantiate and fit.\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert 25 degrees Celsius to Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25 * 1.8 + 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict rentals for 77 degrees Fahrenheit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg.predict(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion:** The scale of the features is irrelevant for prediction in linear regression models. When changing the scale, we simply change our interpretation of the coefficients.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the temp_F column.\n",
    "bikes.drop('temp_F', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Work With Multiple Features\n",
    "---\n",
    "\n",
    "We've demonstrated simple linear regression with one feature to gain an intuition, but the benefit of modeling is the ability to reason about hundreds of features at once. There is no limit to the number of features you can use. However, often a small set of features accounts for most of the variance (assuming there is a linear relationship at all). We will start by using four features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-the-data-part-\"></a>\n",
    "### Visualizing the Data (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature column variables\n",
    "feature_cols = ['temp', 'season', 'weather', 'humidity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a subset of scatterplot matrix using Seaborn.\n",
    "We can use pairplot with the y_vars argument to only show relationships with the `total_rentals` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple scatterplots in Seaborn\n",
    "sns.pairplot(bikes, x_vars=feature_cols, y_vars='total_rentals', kind='reg');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate the same functionality using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple scatterplots in Pandas -- put plots on a 1x4 grid, and force the same Y axis\n",
    "fig, axs = plt.subplots(1, len(feature_cols), sharey=True)\n",
    "for index, feature in enumerate(feature_cols):\n",
    "    bikes.plot(kind='scatter', x=feature, y='total_rentals', ax=axs[index], figsize=(16, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are you seeing anything you didn't expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the season variable using a cross-tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation of season and month\n",
    "pd.crosstab(bikes.season, bikes.index.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore the season variable using a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of rentals, grouped by season\n",
    "bikes.boxplot(column='total_rentals', by='season');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably:\n",
    "\n",
    "- A line can't capture a nonlinear relationship.\n",
    "- The problem is not really with nonlinearity -- *season* is probably better coded as a \"categorical variable\" -- a variable that takes one value from a set of values (like temperatures = {hot, medium, cold}).  There are special techniques for dealing with categorical variables, more on that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at rentals over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of rentals\n",
    "bikes.total_rentals.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does this tell us?\n",
    "\n",
    "There are more rentals in the winter than the spring, but only because the system is experiencing overall growth and the winter months happen to come after the spring months.\n",
    "\n",
    "So does overall growth completely explain the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(bikes['total_rentals'])\n",
    "df['hours_since_open'] = range(0, bikes.shape[0])\n",
    "sns.lmplot(x='hours_since_open', y='total_rentals', data=df, aspect=1.5, scatter_kws={'alpha':0.2});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question for later: how much better -- if at all -- is any model we build from the column featues than this model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at the correlation matrix for the bikes `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix (ranges from 1 to -1)\n",
    "bikes.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a heat map to make it easier to read the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize correlation matrix in Seaborn using a heat map.\n",
    "sns.heatmap(bikes.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;font-size=120%\">What relationships do you notice and which are important for our model building?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding More Features to the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, one variable explained the variance of another; however, more often than not, we will need multiple variables. \n",
    "\n",
    "- For example, a house's price may be best measured by square feet, but a lot of other variables play a vital role: bedrooms, bathrooms, location, appliances, etc. \n",
    "\n",
    "- For a linear regression, we want these variables to be largely independent of one another, but all of them should help explain the y variable.\n",
    "\n",
    "- Eliminating \"colinear\" variables is a separate topic.  For now let's realize that temp and atemp are linearly related so we don't need both\n",
    "- Especially we have to eliminate attributes that are \"definitional\" of the dependent variable!  So registered and casual have to go, but maybe the ratio of registered to total users would be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create another `LinearRegression` instance that is fit using temp, season, weather, and humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of features.\n",
    "feature_cols = ['temp', 'season', 'weather', 'humidity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y.\n",
    "X = bikes[feature_cols]\n",
    "y = bikes.total_rentals\n",
    "\n",
    "# Instantiate and fit.\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# Print the coefficients.\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the linear regression coefficient along with the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair the feature names with the coefficients.\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the coefficients:\n",
    "\n",
    "- Holding all other features fixed, a 1-unit increase in temperature is associated with a rental increase of 7.86 bikes.\n",
    "- Holding all other features fixed, a 1-unit increase in season is associated with a rental increase of 22.5 bikes.\n",
    "- Holding all other features fixed, a 1-unit increase in weather is associated with a rental increase of 6.67 bikes.\n",
    "- Holding all other features fixed, a 1-unit increase in humidity is associated with a rental decrease of 3.12 bikes.\n",
    "\n",
    "Does anything look incorrect and does not reflect reality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Models / Comparing Models\n",
    "---\n",
    "\n",
    "We can make linear models now, but how do we select the \"best\" model to use for our applications? We can answer questions about how well a model fits the observed data, but the definition of \"best\" can depend on other business factors (like cost associated with the model making errors).\n",
    "\n",
    "Two different questions\n",
    "1.  How well does the model fit the observed data (training set)\n",
    "2.  How well does will the model predict future observations in production.\n",
    "\n",
    "Second may differ from the first because\n",
    "1.  The training set was too small -- important features in the population didn't show up enough to be noticed\n",
    "2.  The model found spurious signal in the training set \"total sales is higher on days that begin with 'T'.\" (Overfitting)\n",
    "\n",
    "Looking at the first:\n",
    "* The $R^2$ statistic -- the amount the model reduces variability\n",
    "* Significance tests on the slope coefficient(s) -- if a variable's coefficient is 0, then it is not adding predictive power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics from the Residuals\n",
    "\n",
    "**Residual sum of squares**\n",
    "$$RSS = \\sum_{i=1}^{n} (y_i - \\hat{y})^2$$\n",
    "\n",
    "This is familiar -- how much each prediction deviates from the observed. Remember, the regression chooses coefficients and intercept to minimize this value ($MSE$)\n",
    "\n",
    "**Residual standard error**\n",
    "$$RSE = \\sqrt{ \\frac{1}{n-2} RSS} $$\n",
    "This is the \"average deviation\" of a prediction from the actual value.  It is in terms of $y$ units (e.g. total rentals) so comparing models can be tricky\n",
    "\n",
    "Remember, there is variance in the observations themselves apart from any model;  if the data itself has high variance, then a predictive model will have high variance too.\n",
    "\n",
    "**Total sum of squares**\n",
    "$$TSS = \\sum_{i=1}^n(y_i - \\overline{y})^2$$\n",
    "Standard definition of deviance of observations from the mean -- model independent.\n",
    "\n",
    "**R-squared is proportion of the total variance accounted for by the model**\n",
    "$$R^2 \\quad=\\quad \\frac{(TSS - RSS)}{TSS} \\quad = \\quad 1 - \\frac{RSS}{TSS}$$\n",
    "If RSS = 0 then $R^2$ = 1 which means RSS accounts for 100% of the variance (it perfectly predicts the observed).  If $RSS = TSS$ then the regression does not reduce any variance (worst case model), same as guessing at the mean.\n",
    "\n",
    "$R^2$ is a ratio (unit-free) so can be compared across models, but not clear what a \"good enough\" value is.  At least partially depends on what we know about the underlying system -- contributors to low values might be\n",
    "* Underlying system is not linear -- so linear model will no do well\n",
    "* Underlying system is linear, but we are missing key variates\n",
    "\n",
    "**Adjusted R-squared adjusts for number of variables in the model**\n",
    "$R^2$ increases as more predictors are added to the model, regardless of whether they improve prediction.\n",
    "The adjusted version corrects for this\n",
    "$$ {R}^{2}_{adj \\quad}= \\quad 1-\\left[\\frac{(1-R^{2})(n-1)}{n-p-1}\\right]$$\n",
    "where $n$ is the number of observations and $p$ is the number of variables in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tss(actual):\n",
    "    return ((actual - actual.mean())**2).sum()\n",
    "\n",
    "def rss(actual, predicted):\n",
    "    return ((actual - predicted)**2).sum()\n",
    "\n",
    "def rsquared(actual, predicted):\n",
    "    return (tss(actual) - rss(actual, predicted))/tss(actual)\n",
    "\n",
    "def adjust_rsquared(rquared, n, p):\n",
    "    return 1 - ((1-rsquared) * (n - 1) /(n - p - 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a standard summary from a regression.  We will look at only a couple of pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "bikes['rand'] = 100 + 20*np.random.rand(bikes.shape[0])\n",
    "X = sm.add_constant(bikes[['atemp', 'windspeed', 'season', 'rand']])\n",
    "y = bikes['total_rentals']\n",
    "results = sm.OLS(y, X).fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Highlights of the Regression Results Table**\n",
    "* The F-statistic tests the hypothesis \"all of the coefficients are 0\" -- it's a ratio, and a value of 1 indicates no relationship.  The F-statistic strongly indicates that the model is better at predicting total rentals than using the mean value\n",
    "* For the intercept and coefficients, the $p$ value tests the hypothesis that the parameter is 0.  Values close 0 mean the parameter is unlikely to be 0.  Notice the confidence interval and $p$ value for *rand* -- its coefficient is very likely to be 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A way to get some statistics like adjusted R-squared for a set of columns\n",
    "def statsmodel_summary(dataset, columns):\n",
    "    X = sm.add_constant(dataset[columns])\n",
    "    Y = dataset['total_rentals']\n",
    "    results = sm.OLS(y, X).fit()\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, this took away the random column -- notice R-squared stayed the same\n",
    "# but adjusted R-squared went up a little.\n",
    "\n",
    "statsmodel_summary(bikes, ['atemp', 'windspeed', 'season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"handling-categorical-features\"></a>\n",
    "### Handling Categorical Features\n",
    "\n",
    "scikit-learn expects all features to be numeric. So how do we include a categorical feature in our model?\n",
    "\n",
    "- **Ordered categories:** Transform them to sensible numeric values (example: small=1, medium=2, large=3)\n",
    "- **Unordered categories:** Use dummy encoding (0/1). Here, each possible category would become a separate feature.\n",
    "\n",
    "What are the categorical features in our data set?\n",
    "\n",
    "- **Ordered categories:** `weather` (already encoded with sensible numeric values)\n",
    "- **Unordered categories:** `season` (needs dummy encoding), `holiday` (already dummy encoded), `workingday` (already dummy encoded)\n",
    "\n",
    "For season, we can't simply leave the encoding as 1 = spring, 2 = summer, 3 = fall, and 4 = winter, because that would imply an ordered relationship. Instead, we create multiple dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dummy variables using `get_dummies` from Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_dummies = pd.get_dummies(bikes.season, prefix='season')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the `DataFrame` of `dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print five random rows.\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we actually only need three dummy variables (not four), and thus we'll drop the first dummy variable.\n",
    "\n",
    "Why? Because three dummies captures all of the \"information\" about the season feature, and implicitly defines spring (season 1) as the baseline level.\n",
    "\n",
    "This circles back to the concept multicollinearity, except instead of one feature being highly correlated to another, the information gained from three features is directly correlated to the fourth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_dummies.drop(season_dummies.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinspect the `DataFrame` of `dummies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print five random rows.\n",
    "season_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, if you have a categorical feature with k possible values, you create k-1 dummy variables.\n",
    "\n",
    "If that's confusing, think about why we only need one dummy variable for `holiday`, not two dummy variables (`holiday_yes` and `holiday_no`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We now need to concatenate the two `DataFrames` together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the original DataFrame and the dummy DataFrame (axis=0 means rows, axis=1 means columns).\n",
    "bikes_dummies = pd.concat([bikes, season_dummies], axis=1)\n",
    "\n",
    "# Print 5 random rows.\n",
    "bikes_dummies.sample(n=5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rerun the linear regression with dummy variables included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include dummy variables for season in the model.\n",
    "feature_cols = ['atemp', 'windspeed', 'season_2', 'season_3', 'season_4']\n",
    "X = bikes_dummies[feature_cols]\n",
    "y = bikes_dummies.total_rentals\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "list(zip(feature_cols, linreg.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Same model with dummy variables -- R-squared and adjusted R-squared both improve\n",
    "statsmodel_summary(bikes_dummies, ['atemp', 'windspeed', 'season_2', 'season_3', 'season_4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret the season coefficients? They are measured against the baseline (spring):\n",
    "\n",
    "- Holding all other features fixed, summer is associated with a rental decrease of 3.39 bikes compared to the spring.\n",
    "- Holding all other features fixed, fall is associated with a rental decrease of 41.7 bikes compared to the spring.\n",
    "- Holding all other features fixed, winter is associated with a rental increase of 64.4 bikes compared to the spring.\n",
    "\n",
    "Would it matter if we changed which season was defined as the baseline?\n",
    "\n",
    "- No, it would simply change our interpretation of the coefficients.\n",
    "\n",
    "In most situations, it is best to have the dummy that is your baseline be the category that has the largest representation.\n",
    "\n",
    "**Important:** Dummy encoding is relevant for all machine learning models, not just linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare adjusted R-squared for various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  This is worst case scenario -- rand should have no predictive value\n",
    "statsmodel_summary(bikes, ['rand'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the full model -- we see an improvement -- only question is whether we could remove certain \n",
    "#  variables and improve adjusted R-squared.  Not likely since the \"penalty\" for adding a variable\n",
    "#  seems to be low\n",
    "full_model_columns = ['weather', 'season_2', 'season_3', 'season_4', 'holiday', 'workingday', 'windspeed', 'temp', 'humidity']\n",
    "statsmodel_summary(bikes_dummies, full_model_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Next Step -- Feature Engineering\n",
    "* Choose additional features, that *might* be relevant.  For example, hours since opening, hour of the day (extracted from timestamp)\n",
    "* Look for interaction effects -- for example, the influence of temperature on rentals might depend on the season\n",
    "\n",
    "No good objective methodology for doing that, but evaluation is always to compare the model with the variable(s) to the model without them, evaluate on the basis of Adjusted R^2 or other metric.\n",
    "\n",
    "##### Summary:  Comparing Linear Regression With Other Models\n",
    "\n",
    "Advantages of linear regression:\n",
    "\n",
    "- Simple to explain.\n",
    "- Highly interpretable.\n",
    "- Model training and prediction are fast.\n",
    "- No tuning is required (excluding regularization).\n",
    "- Features don't need scaling.\n",
    "- Can perform well with a small number of observations.\n",
    "- Well understood.\n",
    "\n",
    "Disadvantages of linear regression:\n",
    "\n",
    "- Presumes a linear relationship between the features and the response.\n",
    "- Performance is (generally) not competitive with the best supervised learning methods due to high bias.\n",
    "- Can't automatically learn feature interactions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
