{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors (KNN) Classification\n",
    "---\n",
    "\n",
    "K-nearest neighbors classification is (as its name implies) a classification model that uses the \"K\" most similar observations in order to make a prediction.\n",
    "\n",
    "KNN is a supervised learning method; therefore, the training data must have known target values.\n",
    "\n",
    "The process of of prediction using KNN is fairly straightforward:\n",
    "\n",
    "1. Pick a value for K.\n",
    "2. Search for the K observations in the data that are \"nearest\" to the measurements of the unknown iris.\n",
    "    - Euclidian distance is often used as the distance metric, but other metrics are allowed.\n",
    "3. Use the most popular response value from the K \"nearest neighbors\" as the predicted response value for the unknown iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualizations below show how a given area can change in its prediction as K changes.\n",
    "\n",
    "- This is simulated data with two predictors\n",
    "- Colored points represent true values and colored areas represent a **prediction space**. (This is called a Voronoi Diagram.)\n",
    "- Each prediction space is wgere the majority of the \"K\" nearest points are the color of the space.\n",
    "- To predict the class of a new point, we guess the class corresponding to the color of the space it lies in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classification Map for Iris (K=1)\n",
    "\n",
    "![1NN classification map](iris_01nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classification Map for Iris (K=5)\n",
    "\n",
    "![5NN classification map](iris_05nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classification Map for Iris (K=15)\n",
    "\n",
    "![15NN classification map](iris_15nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN Classification Map for Iris (K=50)\n",
    "\n",
    "![50NN classification map](iris_50nn_map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that, as K increases, the classification spaces' borders become more distinct. However, you can also see that the spaces are not perfectly pure when it comes to the known elements within them.\n",
    "\n",
    "**How are outliers affected by K?** As K increases, outliers are \"smoothed out\". Look at the above three plots and notice how outliers strongly affect the prediction space when K=1. When K=50, outliers no longer affect region boundaries. This is a classic bias-variance tradeoff -- with increasing K, the bias increases but the variance decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:blue;font-size:125%\">\n",
    "- What happens when K $\\rightarrow$ number of points in the sample?\n",
    "</div>\n",
    "<div style=\"color:blue;font-size:125%\">\n",
    "- What is the best value for K?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NBA Position KNN Classifier\n",
    "\n",
    "This dataset containing the 2015 season statistics for ~500 NBA players. The columns we'll use for features (and the target 'pos') are:\n",
    "\n",
    "| Column | Meaning |\n",
    "| ---    | ---     |\n",
    "| pos | C: Center. F: Front. G: Guard |\n",
    "| ast | Assists per game | \n",
    "| stl | Steals per game | \n",
    "| blk | Blocks per game |\n",
    "| tov | Turnovers per game | \n",
    "| pf  | Personal fouls per game | \n",
    "\n",
    "For information about the other columns, see [this glossary](https://www.basketball-reference.com/about/glossary.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the NBA data into a DataFrame.\n",
    "import pandas as pd\n",
    "\n",
    "path = 'NBA_players_2015.csv'\n",
    "nba = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map positions to numbers\n",
    "nba['pos_num'] = nba.pos.map({'C':0, 'F':1, 'G':2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix (X).\n",
    "feature_cols = ['ast', 'stl', 'blk', 'tov', 'pf']\n",
    "X = nba[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create response vector (y).\n",
    "y = nba.pos_num\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"using-the-traintest-split-procedure-k\"></a>\n",
    "### Using the Train/Test Split Procedure (K=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split X and y into training and testing sets (using `random_state` for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train the model on the training set (using K=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  `algorithm=auto`, `leaf_size`  -- brute force or tree-based contruction algorithm;  does not affect solution; parameters to improve speed\n",
    "*  `minkowski` is a family of distance metrics;  with `p=2` this is Euclidian distance, sum of squared distances\n",
    "*  `weights` alternative to `uniform` is to weight closer points more heavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Test the model on the testing set and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = knn.predict(X_test)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(X, y)\n",
    "y_pred_class = knn.predict(X)\n",
    "print((metrics.accuracy_score(y, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:120%\">Exercise</span>\n",
    "* Train for `n_neighbors=1` on the entire dataset then test it on the entire dataset. What accuracy do you expect to get?  If you get something different, why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Repeating for K=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_class = knn.predict(X_test)\n",
    "print((metrics.accuracy_score(y_test, y_pred_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:120%\">Exercise</span>\n",
    "* Train and test on the entire data set, but using 50-KNN. Would we expect the accuracy to be higher, lower, or the same as compared to 1-KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Comparing Testing Accuracy With Null Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Null accuracy is the accuracy that can be achieved by **always predicting the most frequent class**. For example, if most players are Centers, we would always predict Center.\n",
    "\n",
    "The null accuracy is a benchmark against which you may want to measure every classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the class distribution from the training set.\n",
    "\n",
    "Remember that we are comparing KNN to this simpler model. So, we must find the most frequent class **of the training set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_class = y_test.value_counts().index[0]\n",
    "\n",
    "print(y_test.value_counts())\n",
    "print(most_freq_class)\n",
    "print(150 / (150 + 140 + 68))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute null accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()[most_freq_class] / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning a KNN Model\n",
    "\n",
    "Tuning means find an optimal value for K -- what does optimal mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(1,478):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X,y)\n",
    "    pred = knn.predict(X)\n",
    "    score = float(sum(pred == y)) / len(y)\n",
    "    scores.append([k, score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(scores,columns=['k','score'])\n",
    "data.plot.line(x='k',y='score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** As K increases, why does the accuracy fall?\n",
    "\n",
    "**Answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Search for the \"best\" value of K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TRAINING ACCURACY and TESTING ACCURACY for K=1 through 358.\n",
    "\n",
    "k_range = list(range(1, 359))\n",
    "training_accuracies = []\n",
    "testing_accuracies = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    y_pred_class = knn.predict(X_train)\n",
    "    training_accuracy = metrics.accuracy_score(y_train, y_pred_class)\n",
    "    training_accuracies.append(training_accuracy)\n",
    "    \n",
    "    # Calculate testing error.\n",
    "    y_pred_class = knn.predict(X_test)\n",
    "    testing_accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    testing_accuracies.append(testing_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of K, training error, and testing error.\n",
    "column_dict = {'K': k_range, 'training accuracy':training_accuracies, 'testing accuracy':testing_accuracies}\n",
    "df = pd.DataFrame(column_dict).set_index('K').sort_index(ascending=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between K (HIGH TO LOW) and TESTING ACCURACY.\n",
    "df.plot(y='testing accuracy');\n",
    "plt.xlabel('Value of K for KNN');\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum testing error and the associated K value.\n",
    "df.sort_values('testing accuracy', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method:\n",
    "max(list(zip(testing_accuracies, k_range)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"training-error-versus-testing-error\"></a>\n",
    "### Training Accuracy Versus Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between K (HIGH TO LOW) and both TRAINING Accuracy and TESTING Accuracy.\n",
    "df.plot();\n",
    "plt.xlabel('Value of K for KNN');\n",
    "plt.ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remember that model complexity is greatest at $K=1$ -- as $K$ gets larger the model tends toward \"predict at the mode\"\n",
    "\n",
    "- **Training accuracy** increases as model complexity increases (lower value of K).\n",
    "- **Testing accuracy** will tend to be low (overfitting, too much complexity) then increase, then decrease (too little complexity)\n",
    "\n",
    "Evaluating the training and testing accuracy is important. For example:\n",
    "\n",
    "- If the training accuracy is much higher than the test accuracy, then our model is likely overfitting. \n",
    "- If the test accuracy starts decreasing as we vary a hyperparameter (K), we may be overfitting.\n",
    "- If either accuracy plateaus, our model is likely underfitting (not complex enough)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Making Predictions on Out-of-Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given the statistics of a (truly) unknown NBA player, how do we predict his position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Instantiate the model with the best-known parameters.\n",
    "knn = KNeighborsClassifier(n_neighbors=14)\n",
    "\n",
    "# Re-train the model with X and y (not X_train and y_train). Why?\n",
    "knn.fit(X, y)\n",
    "\n",
    "# Make a prediction for an out-of-sample observation.\n",
    "knn.predict(np.array([2, 1, 0, 1, 2]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remember any classifier does estimation followed by choice.\n",
    "##   Often useful to know that underlying probabilities predicted.\n",
    "##     \n",
    "knn.predict_proba(np.array([2, 1, 0, 1, 2]).reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What could we conclude?\n",
    "\n",
    "- When using KNN on this data set with these features, the **best value for K** is likely to be around 14.\n",
    "- Given the statistics of an **unknown player**, we estimate that we would be able to correctly predict his position about 74% of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"standardizing-features\"></a>\n",
    "## Standardizing Features\n",
    "---\n",
    "\n",
    "There is one major issue that applies to many machine learning models: They are sensitive to feature scale. \n",
    "\n",
    "\n",
    "\n",
    "> KNN in particular is sensitive to feature scale because it (by default) uses the Euclidean distance metric. To determine closeness, Euclidean distance sums the square difference along each axis. So, if one axis has large differences and another has small differences, the former axis will contribute much more to the distance than the latter axis.\n",
    "\n",
    "This means that it matters whether our feature are centered around zero and have similar variance to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;size:120%\">Exercise</span>\n",
    "* Our $X$ variables are\n",
    "  * `ast` -- assists\n",
    "  * `stl` -- steals\n",
    "  * `blk` -- blocked shots\n",
    "  * `tov` -- turnovers\n",
    "  * `pf`  -- personal fouls\n",
    "  \n",
    "Suppose the data set had been coded as follows\n",
    "  * `pf` was measured on a thousand fold scale;  1000, 2000, 3000, etc\n",
    "  * `blk` was measured on a scale starting at -100 and increasing by 0.01;  0 => -100, 1 => -99.9\n",
    "\n",
    "Intuitively, what will be the effect of making `pf` bigger?\n",
    "\n",
    "Try this using rescaled values for `pf` and `blk` as above.  Transform the variables, re-train the model at $K=14$ and compare the predictions to those made by the original model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xw = X.copy()\n",
    "Yw = y.copy()\n",
    "Xw[\"pf\"] = Xw[\"pf\"] * 1000\n",
    "Xw[\"blk\"] = -100 + Xw[\"blk\"]* .01\n",
    "knn2 = KNeighborsClassifier(n_neighbors=14)\n",
    "knn2.fit(Xw, Yw)\n",
    "# Calculate testing error.\n",
    "y_pred_class_2 = knn2.predict(Xw)\n",
    "y_pred_class = knn.predict(X)\n",
    "agree = y_pred_class == y_pred_class_2\n",
    "print(agree[0:20])\n",
    "print(sum(agree)/len(agree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization:  Convert all attributes to have a mean of zero and variance of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"use-standardscaler-to-standardize-our-data\"></a>\n",
    "### Use `StandardScaler` to Standardize our Data\n",
    "\n",
    "StandardScaler standardizes our data by subtracting the mean from each feature and dividing by its standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate feature matrix and response for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix (X).\n",
    "feature_cols = ['ast', 'stl', 'blk', 'tov', 'pf']\n",
    "\n",
    "X = nba[feature_cols]\n",
    "y = nba.pos_num  # Create response vector (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Create the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Instantiate and fit `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(copy=True)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[:,4].mean())\n",
    "print(X_train[:,4].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a KNN model and look at the testing error.\n",
    "Can you find a number of neighbors that improves our results from before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = list(range(1, 348))\n",
    "training_accuracies = []\n",
    "testing_accuracies = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Calculate training accuracy\n",
    "    y_pred_class = knn.predict(X)\n",
    "    training_accuracy = metrics.accuracy_score(y, y_pred_class)\n",
    "    training_accuracies.append(training_accuracy)\n",
    "    \n",
    "    # Calculate testing error.\n",
    "    y_pred_class = knn.predict(X_test)\n",
    "    testing_accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    testing_accuracies.append(testing_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate testing error.\n",
    "best_k = ??\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred_class = knn.predict(X_test)\n",
    "testing_accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "\n",
    "print(testing_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-knn-with-other-models\"></a>\n",
    "## Comparing KNN With Other Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of KNN:**\n",
    "\n",
    "- It's simple to understand and explain.\n",
    "- Model training is fast.\n",
    "- It can be used for classification and regression (for regression, take the average value of the K nearest points!).\n",
    "- Being a non-parametric method, it is often successful in classification situations where the decision boundary is very irregular.\n",
    "\n",
    "**Disadvantages of KNN:**\n",
    "\n",
    "- It must store all of the training data.\n",
    "- Its prediction phase can be slow when n is large.\n",
    "- It is sensitive to irrelevant features.\n",
    "- It is sensitive to the scale of the data.\n",
    "- Accuracy is (generally) not competitive with the best supervised learning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
