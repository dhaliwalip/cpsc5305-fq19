{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Pandas for Exploratory Data Analysis\n",
    "\n",
    "_Author: Kevin Markham (Washington, D.C.)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Define what Pandas is and how it relates to data science.\n",
    "- Manipulate Pandas `DataFrames` and `Series`.\n",
    "- Filter and sort data using Pandas.\n",
    "- Manipulate `DataFrame` columns.\n",
    "- Know how to handle null and missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "### Data Preparation, EDA, and the Data Science Workflow\n",
    "\n",
    "**Basic Workflow**\n",
    " * Define and frame your question(s)\n",
    " * Collect data relevant to your question(s)\n",
    " * **Prepare and analyze the data**\n",
    "   * Clean and understand\n",
    "   * Analyses\n",
    "      * Shape and meaning\n",
    "      * Hypothesis test\n",
    " * Conclusion and presentation\n",
    " \n",
    " Preparation and Simultaneous Analysis\n",
    "   *  Data comes in from multiple sources / files\n",
    "   *  Different meaning in different files\n",
    "      *  Quality of kitchen is high\n",
    "   *  Same attribute stated differently\n",
    "      *  Date of birth versus age\n",
    "      *  Marital status coded differently or different values\n",
    "   * Noise in the file\n",
    "      *  Misspellings, transpositions, etc. etc. etc.\n",
    "      *  Obviously incorrect values\n",
    "          * 1000 year old person\n",
    "          * House with 500 bathrooms\n",
    "          \n",
    "  Goal of This Phase\n",
    "    * Understand each data file -- what are the attributes and what do they mean\n",
    "       * For example, \"date last seen\" for person's address\n",
    "    * Identify and fix noise\n",
    "       * This will always require some EDA\n",
    "    * Deal with missing values\n",
    "    * Merge different sources to get one view on data\n",
    "    * Start getting a sense of how that data can be used to answer the question\n",
    "    \n",
    " ------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"pandas\"></a>\n",
    "\n",
    "## What Is Pandas?\n",
    "\n",
    "- **Objective:** Define what Pandas is and how it relates to data science.\n",
    "\n",
    "Pandas is a Python library that primarily adds two new datatypes to Python: `DataFrame` and `Series`.\n",
    "\n",
    "- A `Series` is a sequence of items, where each item has a unique label (called an `index`).\n",
    "- A `DataFrame` is a table of data. Each row has a unique label (the `row index`), and each column has a unique label (the `column index`).\n",
    "- Note that each column in a `DataFrame` can be considered a `Series` (`Series` index).\n",
    "\n",
    "> Behind the scenes, these datatypes use the NumPy (\"Numerical Python\") library. NumPy primarily adds the `ndarray` (n-dimensional array) datatype to Pandas. An `ndarray` is similar to a Python list â€” it stores ordered data. However, it differs in three respects:\n",
    "> - Each element has the same datatype (typically fixed-size, e.g., a 32-bit integer).\n",
    "> - Elements are stored contiguously (immediately after each other) in memory for fast retrieval.\n",
    "> - The total size of an `ndarray` is fixed.\n",
    "\n",
    "> Storing `Series` and `DataFrame` data in `ndarray`s makes Pandas faster and uses less memory than standard Python datatypes. Many libraries (such as scikit-learn) accept `ndarray`s as input rather than Pandas datatypes, so we will frequently convert between them.\n",
    "\n",
    "\n",
    "### Using Pandas\n",
    "\n",
    "Pandas is frequently used in data science because it offers a large set of commonly used functions, is relatively fast, and has a large community. Because many data science libraries also use NumPy to manipulate data, you can easily transfer data between libraries (as we will often do in this class!).\n",
    "\n",
    "Pandas is a large library that typically takes a lot of practice to learn. It heavily overrides Python operators, resulting in odd-looking syntax. For example, given a `DataFrame` called `cars` which contains a column `mpg`, we might want to view all cars with mpg over 35. To do this, we might write: `cars[cars['mpg'] > 35]`. In standard Python, this would most likely give a syntax error. (**Challenge:** Using only built-in datatypes, can you define `cars` and `mpg` to make this expression valid?)\n",
    "\n",
    "Pandas also highly favors certain patterns of use. For example, looping through a `DataFrame` row by row is highly discouraged. Instead, Pandas favors using **vectorized functions** that operate column by column. (This is because each column is stored separately as an `ndarray`, and NumPy is optimized for operating on `ndarray`s.)\n",
    "\n",
    "Do not be discouraged if Pandas feels overwhelming. Gradually, as you use it, you will become familiar with which methods to use and the \"Pandas way\" of thinking about and manipulating data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Methods and Attributes\n",
    "\n",
    "Pandas `DataFrame`s are Pandas class objects and therefore come with attributes and methods. To access these, follow the variable name with a dot. For example, given a `DataFrame` called `users`:\n",
    "\n",
    "  - users.index -- accesses the `index` attribute -- note there are no parentheses. attributes are not callable\n",
    "  - users.head() -- calls the `head` method (since there are open/closed parentheses)\n",
    "  - users.head(10) -- calls the `head` method with parameter `10`, indicating the first 10 rows. this is the same as:\n",
    "  - users.head(n=10) -- calls the `head` method, setting the named parameter `n` to have a value of `10`.\n",
    "\n",
    "We know that the `head` method accepts one parameter with an optional name of `n` because it is in the documentation for that method. Let's see how to view the documentation next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Documentation\n",
    "\n",
    "There are a few ways to find more information about a method.\n",
    "\n",
    "**Method 1:** In Jupyter, you can quickly view documentation for a method by following the method name by a `?`, as follows:\n",
    "\n",
    "```\n",
    "users.head?\n",
    "```\n",
    "\n",
    "> ```\n",
    "Signature: users.head(n=5)\n",
    "Docstring: Returns first n rows\n",
    "```\n",
    "\n",
    "Notice that we would normally invoke this method by calling `users.head(5)`. One quirk of IPython is that the `?` symbol must be the last character in the cell. Otherwise, it might not work.\n",
    "\n",
    "> The `?` is a shortcut for the built-in Python function `help`, which returns the method's docstring. For example:\n",
    "> ```\n",
    "help(users.head)\n",
    "```\n",
    "\n",
    "**Method 2:** You can also search online for the phrase \"`DataFrame head`\", since you are calling the method `head` on the `users` object, which happens to be a `DataFrame`. (`type(users) => pandas.DataFrame`)\n",
    "\n",
    "You can alternatively search online for `pandas head`, but be careful! `DataFrame` and `Series` both have a `head` method, so make sure you view the documentation for the correct one since they might be called differently. You will know you are looking at the correct documentation page because it will say `DataFrame.head` at the top, instead of `Series.head`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pandas into Python\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "#### Quick Note on Modules and Imports\n",
    "  * A **module** is just a file full of Python code. Usually class and function definitions\n",
    "  * There are three kinds of modules\n",
    "    * Built-in modules, ship with standard Python distribution.  For example ``os`` is a built-module with some operating system specific functions\n",
    "    * Installed modules, which you will typically get with ``conda install``.  For example ``pandas`` is an installed module\n",
    "    * Your own module -- you just put Python code in a file ``foo.py`` and say ``install foo``\n",
    "  * Modules define a \"namespace\" -- for example when I import pandas and it defines a class ``DataFrame`` its name is really ``pandas.DataFrame``\n",
    "  * The ``as pd`` is just an alias so we can say ``pd.DataFrame``\n",
    "  * You can import specific functions from a module, and alias them, as in the second import\n",
    "  \n",
    "  * The last line above is a \"magic command\" to configure the ``matplotlib`` library to render its plots in the notebook rather than opening a new window\n",
    "    \n",
    "------------------------------\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"reading-files\"></a>\n",
    "### Reading Files, Selecting Columns, and Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_table('./data/user.tbl', sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examine the users data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users                   # Print the first 30 and last 30 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(users)             # DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head()            # Print the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.head(10)          # Print the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.tail()            # Print the last five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # The row index (aka \"the row labels\" â€” in this case integers)\n",
    "users.index            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column names (which is \"an index\")\n",
    "users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datatypes of each column â€” each column is stored as an ndarray, which has a datatype\n",
    "users.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of rows and columns\n",
    "users.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All values as a NumPy array\n",
    "users.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concise summary (including memory usage) â€” useful to quickly see if nulls exist\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Select or index data.**<br>\n",
    "Pandas `DataFrame`s have structural similarities with Python-style lists and dictionaries.  \n",
    "In the example below, we select a column of data using the name of the column in a similar manner to how we select a dictionary value with the dictionary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a column â€” returns a Pandas Series (essentially an ndarray with an index)\n",
    "users['gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame columns are Pandas Series.\n",
    "type(users['gender'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one column using the DataFrame attribute.\n",
    "users.gender\n",
    "\n",
    "# While a useful shorthand, these attributes only exist\n",
    "# if the column name has no punctuations or spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summarize (describe) the data.**<br>\n",
    "Pandas has a bunch of built-in methods to quickly summarize your data and provide you with a quick general understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe all numeric columns.\n",
    "#    Why would it make sense for the default to be numeric only?\n",
    "users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe all columns of type object (can include multiple types).\n",
    "users.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe all columns, including non-numeric.\n",
    "users.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe a single column â€” recall that \"users.gender\" refers to a Series.\n",
    "users.gender.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the ages.\n",
    "users.age.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about this -- remember zip_code column is of type object\n",
    "users.zip_code.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a histogram of a column (the distribution of ages).\n",
    "users.age.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count the number of occurrences of each value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.gender.value_counts()   # Most useful for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.gender.value_counts().plot(kind='bar')     # Quick plot by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also be used with numeric variables\n",
    "#   Try .sort_index() to sort by indices or .sort_values() to sort by counts.\n",
    "users.age.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.age.value_counts().sort_index().plot(kind='bar', figsize=(12,12));     # Bigger plot by increasing age\n",
    "plt.xlabel('Age');\n",
    "plt.ylabel('Number of users');\n",
    "plt.title('Number of users per age');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x/y scatter plots -- \n",
    "avc = users.age.value_counts()\n",
    "plt.scatter(avc.index, avc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise-one\"></a>\n",
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read drinks.csv into a DataFrame called \"drinks\".\n",
    "drinks = pd.read_csv('data/drinks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the head and the tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the column indexes, datatypes, and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the beer_servings Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average beer_servings for the entire data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"filtering-and-sorting\"></a>\n",
    "### Filtering and Sorting\n",
    "- **Objective:** Filter and sort data using Pandas.\n",
    "\n",
    "We can use simple operator comparisons on columns to extract relevant or drop irrelevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logical filtering: Only show users with age < 20.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series of Booleansâ€¦\n",
    "# In Pandas, this comparison is performed element-wise on each row of data.\n",
    "young_bool = users.age < 20\n",
    "print(type(young_bool))\n",
    "young_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€¦and use that Series to filter rows.\n",
    "# In Pandas, indexing a DataFrame by a Series of Booleans only selects rows that are True in the Boolean.\n",
    "young_users = users[young_bool]\n",
    "print(type(young_users))\n",
    "young_users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, combine into a single step.\n",
    "users[users.age < 20].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the masked data frame is itself a dataframe,\n",
    "# you can use all the same operations\n",
    "users[users.age < 20].occupation.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logical filtering with multiple conditions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ampersand for `AND` condition\n",
    "# Important: You MUST put parentheses around each expression because `&` has a higher precedence than `<`.\n",
    "users[(users.age < 20) & (users.gender=='M')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipe for `OR` condition\n",
    "# Important: You MUST put parentheses around each expression because `|` has a higher precedence than `<`.\n",
    "users[(users.age < 20) | (users.age > 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preferred alternative to multiple `OR` conditions\n",
    "users[users.occupation.isin(['doctor', 'lawyer'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort a Series.\n",
    "users.age.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort a DataFrame by a single column.\n",
    "users.sort_values('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use descending order instead.\n",
    "users.sort_values('age', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by multiple columns for nested sort\n",
    "users.sort_values(['occupation', 'age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise-two\"></a>\n",
    "### Exercise 2\n",
    "Use the `drinks.csv` or `drinks` `DataFrame` from earlier to complete the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrame to only include European countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter DataFrame to only include European countries with wine_servings > 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average beer_servings for all of Europe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks = pd.read_table('./data/drinks.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which 10 countries have the highest total_litres_of_pure_alcohol.\n",
    "#drinks.sort_values(drinks.total_litres_of_pure_alcohol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"columns\"></a>\n",
    "### Renaming, Adding, and Removing Columns\n",
    "\n",
    "- **Objective:** Manipulate `DataFrame` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename one or more columns in a single output using value mapping.\n",
    "#  Produces a new copy with renamed columns\n",
    "print(drinks.columns)\n",
    "drinks.rename(columns={'beer_servings':'beer', 'wine_servings':'wine'})\n",
    "print(drinks.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename one or more columns in the original DataFrame.\n",
    "#  Replaces names in the current data frame\n",
    "print(drinks.columns)\n",
    "drinks.rename(columns={'beer_servings':'beer', 'wine_servings':'wine'}, inplace=True)\n",
    "print(drinks.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all column names using a list of matching length.\n",
    "drink_cols = ['country', 'beer', 'spirit', 'wine', 'liters', 'continent'] \n",
    "\n",
    "# Replace during file reading (disables the header from the file).\n",
    "drinks = pd.read_csv('data/drinks.csv', header=0, names=drink_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Easy Column Operations**<br>\n",
    "Rather than having to reference indexes and create for loops to do column-wise operations, Pandas is smart and knows that when we add columns together we want to add the values in each row together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column as a function of existing columns.\n",
    "#  This operation *does* change the data frame\n",
    "\n",
    "drinks['servings'] = drinks.beer + drinks.spirit + drinks.wine\n",
    "drinks['mL'] = drinks.liters * 1000\n",
    "\n",
    "drinks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis=0 for rows, 1 for columns\n",
    "drinks.drop('mL', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop multiple columns.\n",
    "drinks.drop(['mL', 'servings'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember those do not change the original data frame\n",
    "drinks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop on the original DataFrame rather than returning a new one.\n",
    "drinks.drop(['mL', 'servings'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"missing-values\"></a>\n",
    "### Handling Missing Values\n",
    "\n",
    "- **Objective:** Know how to handle null and missing values.\n",
    "\n",
    "Sometimes, values will be missing from the source data or as a byproduct of manipulations. It is very important to detect missing data. Missing data can:\n",
    "\n",
    "- Make the entire row ineligible to be training data for a model.\n",
    "- Hint at data-collection errors.\n",
    "- Indicate improper conversion or manipulation.\n",
    "- Actually not be missing â€” it sometimes means \"zero,\" \"false,\" \"not applicable,\" or \"entered an empty string.\"\n",
    "\n",
    "For example, a `.csv` file might have a missing value in some data fields:\n",
    "\n",
    "```\n",
    "tool_name,material,cost\n",
    "hammer,wood,8\n",
    "chainsaw,,\n",
    "wrench,metal,5\n",
    "```\n",
    "\n",
    "When this data is imported, \"null\" values will be stored in the second row (in the \"material\" and \"cost\" columns).\n",
    "\n",
    "> In Pandas, a \"null\" value is either `None` or `np.NaN` (Not a Number). Many fixed-size numeric datatypes (such as integers) do not have a way of representing `np.NaN`. So, numeric columns will be promoted to floating-point datatypes that do support it. For example, when importing the `.csv` file above:\n",
    "\n",
    "> - **For the second row:** `None` will be stored in the \"material\" column and `np.NaN` will be stored in the \"cost\" column. The entire \"cost\" column (stored as a single `ndarray`) must be stored as floating-point values to accommodate the `np.NaN`, even though an integer `8` is in the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values are usually excluded in calculations by default.\n",
    "print(drinks.shape)\n",
    "print(drinks[drinks.continent.notnull()].shape)\n",
    "drinks.continent.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes missing values\n",
    "drinks.continent.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find missing values in a Series.\n",
    "# True if missing, False if not missing\n",
    "drinks.continent.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the missing values â€” sum() works because True is 1 and False is 0.\n",
    "drinks.continent.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True if not missing, False if missing\n",
    "drinks.continent.notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show rows where continent is not missing.\n",
    "drinks[drinks.continent.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding Pandas Axis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sums \"down\" the 0 axis (rows) â€” so, we get the sums of each column\n",
    "drinks.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# axis=0 is the default.\n",
    "drinks.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sums \"across\" the 1 axis (columns) â€” so, we get the sums of numeric values in the row (beer+spirit+wine+liters+â€¦)\n",
    "drinks.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(drinks.head(2))\n",
    "print(\"---\")\n",
    "print(drinks.sum(axis=1).head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find missing values in a `DataFrame`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of Booleans\n",
    "drinks.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the missing values in each column â€” remember by default, axis=0.\n",
    "print((drinks.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks.isnull().sum().plot(kind='bar');         # visually\n",
    "plt.title('Number of null values per column');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dropping Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row if ANY values are missing from any column â€” can be dangerous!\n",
    "drinks.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a row only if ALL values are missing.\n",
    "drinks.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filling Missing Values**<br>\n",
    "You may have noticed that the continent North America (NA) does not appear in the `continent` column. Pandas read in the original data and saw \"NA\", thought it was a missing value, and converted it to a `NaN`, missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values with \"NA\" â€” this is dangerous to do without manually verifying them!\n",
    "drinks.continent.fillna(value='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifies \"drinks\" in-place\n",
    "drinks.continent.fillna(value='NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the missing value filter â€” this is a better approach!\n",
    "drinks = pd.read_csv('../data/drinks.csv', header=0, names=drink_cols, na_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise-three\"></a>\n",
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read ufo.csv into a DataFrame called \"ufo\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the three most common colors reported?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename any columns with spaces so that they don't contain spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reports in VA, what's the most common city?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a DataFrame containing only reports from Arlington, VA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many rows remain if you drop all rows with any missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split-apply-combine\"></a>\n",
    "### Split-Apply-Combine\n",
    "\n",
    "Split-apply-combine is a pattern for analyzing data. Suppose we want to find mean beer consumption per country. Then:\n",
    "\n",
    "- **Split:** We group data by continent.\n",
    "- **Apply:** For each group, we apply the `mean()` function to find the average beer consumption.\n",
    "- **Combine:** We now combine the continent names with the `mean()`s to produce a summary of our findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks = pd.read_csv('data/drinks.csv')\n",
    "drinks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = drinks.groupby('continent')\n",
    "gb.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continent, calculate the mean beer servings.\n",
    "drinks.groupby('continent').beer_servings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continent, calculate the mean of all numeric columns.\n",
    "drinks.groupby('continent').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continent, describe beer servings.\n",
    "drinks.groupby('continent').beer_servings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar, but outputs a DataFrame and can be customized â€” \"agg\" allows you to aggregate results of Series functions\n",
    "#drinks.groupby('continent').beer_servings.agg(['count', 'mean', 'min', 'max'])\n",
    "drinks.groupby('continent').beer_servings.agg(['count', 'mean', 'min', 'max']).sort_values('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each continent, count the number of rows.\n",
    "print((drinks.groupby('continent').continent.count()))\n",
    "print((drinks.continent.value_counts()))   # should be the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exercise-four\"></a>\n",
    "### Exercise 4\n",
    "\n",
    "Use the \"users\" `DataFrame` or \"users\" file in the Data folder to complete the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_table('./data/user.tbl', sep='|')\n",
    "users.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each occupation in \"users\", count the number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each occupation, calculate the mean age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each occupation, calculate the minimum and maximum ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each combination of occupation and gender, calculate the mean age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"multiple-columns\"></a>\n",
    "### Selecting Multiple Columns and Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select multiple columns â€” yet another overload of the DataFrame indexing operator!\n",
    "my_cols = ['City', 'State']     # Create a list of column names...\n",
    "ufo[my_cols]                    # ...and use that list to select columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, combine into a single step (this is a Python list inside of the Python index operator!).\n",
    "ufo[['City', 'State']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `loc` to select columns by name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"loc\" locates the values from the first parameter (colon means \"all rows\"), \n",
    "#  and the column \"City\". Does the same thing as ufo.City but can be more general\n",
    "ufo.loc[:, 'City']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select two columns.\n",
    "ufo.loc[:, ['City', 'State']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a range of columns â€” unlike Python ranges, \n",
    "#  Pandas index ranges INCLUDE the final column in the range.\n",
    "ufo.loc[:, 'City':'State']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"loc\" can also filter rows by \"name\" (the index).\n",
    "# Row 0, all columns\n",
    "rowzero = ufo.loc[0,:]\n",
    "print(type(rowzero))\n",
    "print(rowzero)\n",
    "print(rowzero['State'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows 0/1/2, all columns\n",
    "ufo.loc[0:2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows 0/1/2, range of columns\n",
    "ufo.loc[0:2, 'City':'State'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use \"iloc\" to filter rows and select columns by integer position.\n",
    "# (Remember that rows/columns use indices, so \"iloc\" lets you refer \n",
    "# to indices via their index rather than value)\n",
    "# All rows, columns in position 0/3 (City/State)\n",
    "ufo.iloc[:,[0, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All rows, columns in position 0/1/2/3\n",
    "# Note here it is NOT INCLUDING 4 because this is an integer range, not a Pandas index range!\n",
    "ufo.iloc[:, 0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rows in position 0/1/2, all columns\n",
    "ufo.iloc[0:3, :] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"joining-dataframes\"></a>\n",
    "### Joining (Merging) `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 data/movies.tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_cols = ['movie_id', 'title']\n",
    "movies_filename = 'data/movies.tbl'\n",
    "\n",
    "movies = pd.read_table(movies_filename, encoding=\"latin-1\", sep='|', header=None, names=movie_cols, usecols=[0, 1])\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head data/movie_ratings.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_cols = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "ratings_filename = 'data/movie_ratings.tsv'\n",
    "ratings = pd.read_table(ratings_filename, sep='\\t', header=None, names=rating_cols)\n",
    "ratings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge \"movies\" and \"ratings\" (inner join on \"movie_id\").\n",
    "movie_ratings = pd.merge(movies, ratings)\n",
    "movie_ratings.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movies.shape)\n",
    "print(ratings.shape)\n",
    "print(movie_ratings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the original files we see that\n",
    "  * Lion King has movie ID 71\n",
    "  * In the ratings file there are 220 ratings with that movie ID\n",
    "  * Which is the number of rows in the merged/joined data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!grep 'Lion King' data/movies.tbl\n",
    "!echo \"---------------\"\n",
    "!awk -F\"\\t\" '$2 == \"71\" {print $0}' data/movie_ratings.tsv\n",
    "!echo \"---------------\"\n",
    "!awk -F\"\\t\" '$2 == \"71\" {print $0}' data/movie_ratings.tsv | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings[movie_ratings.movie_id == 71].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"other-features\"></a>\n",
    "### OPTIONAL: Other Commonly Used Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply an arbitrary function to each value of a Pandas column, storing the result in a new column.\n",
    "users['under30'] = users.age.apply(lambda age: age < 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply an arbitrary function to each row of a DataFrame, storing the result in a new column.\n",
    "#  (Remember that, by default, axis=0. Since we want to go row by row, we set axis=1.)\n",
    "users['under30male'] = users.apply(lambda row: row.age < 30 and row.gender == 'M', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map existing values to a different set of values.\n",
    "users['is_male'] = users.gender.map({'F':0, 'M':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all instances of a value in a column (must match entire value).\n",
    "ufo.State.replace('Fl', 'FL', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String methods are accessed via \"str\".\n",
    "ufo.State.str.upper()                               # Converts to upper case\n",
    "# checks for a substring\n",
    "ufo[ufo['Colors'].str.contains('RED', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a string to the datetime format (this is often slow â€” consider doing it in the \"read_csv()\" method.)\n",
    "ufo['Time'] = pd.to_datetime(ufo.Time)\n",
    "ufo.Time.dt.hour                        # Datetime format exposes convenient attributes\n",
    "(ufo.Time.max() - ufo.Time.min()).days  # Also allows you to do datetime \"math\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo = pd.read_csv('data/ufo.csv')\n",
    "ufo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo.Time.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set and then remove an index.\n",
    "ufo.set_index('Time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ufo.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo.Time.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype of a column.\n",
    "drinks['beer'] = drinks.beer.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy variables are used for learning algorithms to deal with categorical variables.\n",
    "If I have an attribute **size** that can only have three state, small medium and large I create binary variables for example **size_small** and **size_medium** then if the size is small I set the dummies to be (1,0) if the size is medium I set (0,1) and if the size is large I set (0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy variables for \"continent\" and exclude first dummy column.\n",
    "continent_dummies = pd.get_dummies(drinks.continent, prefix='cont').iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continent_dummies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate two DataFrames (axis=0 for rows, axis=1 for columns).\n",
    "drinks = pd.concat([drinks, continent_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"uncommon-features\"></a>\n",
    "### OPTIONAL: Other Less-Used Features of Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = pd.read_table(\"data/user.tbl\", sep='|')\n",
    "user.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting duplicate rows\n",
    "print(user.shape[0])\n",
    "dedup = user.drop_duplicates(['age', 'gender', 'occupation', 'zip_code'])\n",
    "print(dedup.shape[0])     # Drop duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above we had to choose columns because the **user_id** column was unique so there were no true duplicates.  But if **user_id** really is unique, then it can be used as an index.  Once it is an index we can dedupe on all columns and find 7 duplicates."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "user = pd.read_table(\"data/user.tbl\", sep='|')\n",
    "user.set_index('user_id', inplace=True)\n",
    "user.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks = pd.read_csv('data/drinks.csv')\n",
    "drinks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a range of values into descriptive groups.\n",
    "drinks['beer_level'] = 'low'    # Initially set all values to \"low\"\n",
    "drinks.loc[drinks.beer_servings.between(101, 200), 'beer_level'] = 'med'     # Change 101-200 to \"med\"\n",
    "drinks.loc[drinks.beer_servings.between(201, 400), 'beer_level'] = 'high'    # Change 201-400 to \"high\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a cross-tabulation of two Series.\n",
    "pd.crosstab(drinks.continent, drinks.beer_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "For learning applications it is important to know whether a variable is just a string -- like \"name\" that could take any value, or categorical like \"beer_level\" that can only take three values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"beer_level\" into the \"category\" datatype.\n",
    "drinks['beer_level'] = pd.Categorical(drinks.beer_level, categories=['low', 'med', 'high'])\n",
    "drinks.sort_values('beer_level')   # Sorts by the categorical ordering (low to high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit which rows are read when reading in a file â€” useful for large files!\n",
    "pd.read_csv('../data/drinks.csv', nrows=10)           # Only read first 10 rows\n",
    "pd.read_csv('../data/drinks.csv', skiprows=[1, 2])    # Skip the first two rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a DataFrame out to a .csv\n",
    "drinks.to_csv('drinks_updated.csv')                 # Index is used as first column\n",
    "drinks.to_csv('drinks_updated.csv', index=False)    # Ignore index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a dictionary.\n",
    "pd.DataFrame({'capital':['Montgomery', 'Juneau', 'Phoenix'], 'state':['AL', 'AK', 'AZ']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a list of lists.\n",
    "pd.DataFrame([['Montgomery', 'AL'], ['Juneau', 'AK'], ['Phoenix', 'AZ']], columns=['capital', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a DataFrame.\n",
    "import NumPy as np\n",
    "mask = np.random.rand(len(drinks)) < 0.66   # Create a Series of Booleans\n",
    "train = drinks[mask]                        # Will contain around 66% of the rows\n",
    "test = drinks[~mask]                        # Will contain the remaining rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the maximum number of rows and columns printed ('None' means unlimited).\n",
    "pd.set_option('max_rows', None)     # Default is 60 rows\n",
    "pd.set_option('max_columns', None)  # Default is 20 columns\n",
    "print(drinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset options to defaults.\n",
    "pd.reset_option('max_rows')\n",
    "pd.reset_option('max_columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the options temporarily (settings are restored when you exit the \"with\" block).\n",
    "with pd.option_context('max_rows', None, 'max_columns', None):\n",
    "    print(drinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "### Summary\n",
    "\n",
    "Believe it or not, we've only barely touched the surface of everything that Pandas offers. Don't worry if you don't remember most of it â€” for now, just knowing what exists is key. Remember that the more you use Pandas to manipulate data, the more of these functions you will take interest in, look up, and remember.\n",
    "\n",
    "In this notebook, the most important things to familiarize yourself with are the basics:\n",
    "- Manipulating `DataFrames` and `Series`\n",
    "- Filtering columns and rows\n",
    "- Handling missing values\n",
    "- Split-apply-combine (this one takes some practice!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
